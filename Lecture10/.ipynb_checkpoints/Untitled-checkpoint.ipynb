{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Text Analytics\n",
    "\n",
    "### Data Science 350\n",
    "### Stephen Elston\n",
    "\n",
    "## Introduction \n",
    "\n",
    "This notebook contains a tutorial introduction to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>sentiment</th><th scope=col>tweets</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>1                                                                                                                    </td><td>@elephantbird Hey dear, Happy Friday to You  Already had your rice's bowl for lunch ?                                </td></tr>\n",
       "\t<tr><td>1                                                                                                                    </td><td>Ughhh layin downnnn    Waiting for zeina to cook breakfast                                                           </td></tr>\n",
       "\t<tr><td>0                                                                                                                    </td><td>@greeniebach I reckon he'll play, even if he's not 100%...but i know nothing!! ;) It won't be the same without him.  </td></tr>\n",
       "\t<tr><td>0                                                                                                                    </td><td>@vaLewee I know!  Saw it on the news!                                                                                </td></tr>\n",
       "\t<tr><td>0                                                                                                                    </td><td>very sad that http://www.fabchannel.com/ has closed down. One of the few web services that I've used for over 5 years</td></tr>\n",
       "\t<tr><td>0                                                                                                                    </td><td>@Fearnecotton who sings 'I Remember'? i alwaysss hear it on Radio 1 but never catch the artist                       </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " sentiment & tweets\\\\\n",
       "\\hline\n",
       "\t 1                                                                                                                     & @elephantbird Hey dear, Happy Friday to You  Already had your rice's bowl for lunch ?                                \\\\\n",
       "\t 1                                                                                                                     & Ughhh layin downnnn    Waiting for zeina to cook breakfast                                                           \\\\\n",
       "\t 0                                                                                                                       & @greeniebach I reckon he'll play, even if he's not 100\\%...but i know nothing!! ;) It won't be the same without him.  \\\\\n",
       "\t 0                                                                                                                     & @vaLewee I know!  Saw it on the news!                                                                                \\\\\n",
       "\t 0                                                                                                                     & very sad that http://www.fabchannel.com/ has closed down. One of the few web services that I've used for over 5 years\\\\\n",
       "\t 0                                                                                                                     & @Fearnecotton who sings 'I Remember'? i alwaysss hear it on Radio 1 but never catch the artist                       \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "sentiment | tweets | \n",
       "|---|---|---|---|---|---|\n",
       "| 1                                                                                                                     | @elephantbird Hey dear, Happy Friday to You  Already had your rice's bowl for lunch ?                                 | \n",
       "| 1                                                                                                                     | Ughhh layin downnnn    Waiting for zeina to cook breakfast                                                            | \n",
       "| 0                                                                                                                     | @greeniebach I reckon he'll play, even if he's not 100%...but i know nothing!! ;) It won't be the same without him.   | \n",
       "| 0                                                                                                                     | @vaLewee I know!  Saw it on the news!                                                                                 | \n",
       "| 0                                                                                                                     | very sad that http://www.fabchannel.com/ has closed down. One of the few web services that I've used for over 5 years | \n",
       "| 0                                                                                                                     | @Fearnecotton who sings 'I Remember'? i alwaysss hear it on Radio 1 but never catch the artist                        | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  sentiment\n",
       "1 1        \n",
       "2 1        \n",
       "3 0        \n",
       "4 0        \n",
       "5 0        \n",
       "6 0        \n",
       "  tweets                                                                                                               \n",
       "1 @elephantbird Hey dear, Happy Friday to You  Already had your rice's bowl for lunch ?                                \n",
       "2 Ughhh layin downnnn    Waiting for zeina to cook breakfast                                                           \n",
       "3 @greeniebach I reckon he'll play, even if he's not 100%...but i know nothing!! ;) It won't be the same without him.  \n",
       "4 @vaLewee I know!  Saw it on the news!                                                                                \n",
       "5 very sad that http://www.fabchannel.com/ has closed down. One of the few web services that I've used for over 5 years\n",
       "6 @Fearnecotton who sings 'I Remember'? i alwaysss hear it on Radio 1 but never catch the artist                       "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Read the tweet data set\n",
    "tweets = read.csv('Binary Classification_ Twitter sentiment analysis.csv', \n",
    "                   header = TRUE, stringsAsFactors = FALSE)\n",
    "colnames(tweets) <- c(\"sentiment\", \"tweets\") # Set the column names\n",
    "tweets[, 'sentiment'] = ifelse(tweets$sentiment == 4, 1, 0)  # set sentiment to {0,1}\n",
    "head(tweets) # Have a look at the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in library(tm): there is no package called 'tm'\n",
     "output_type": "error",
     "traceback": [
      "Error in library(tm): there is no package called 'tm'\nTraceback:\n",
      "1. library(tm)",
      "2. stop(txt, domain = NA)"
     ]
    }
   ],
   "source": [
    "## Create a tm text corpus from the tweets\n",
    "library(tm)  ## tm package for text mining\n",
    "tweet.corpus <- Corpus(VectorSource(tweets['tweets']))\n",
    "class(tweet.corpus) # What is the class of the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in eval(expr, envir, enclos): could not find function \"tm_map\"\n",
     "output_type": "error",
     "traceback": [
      "Error in eval(expr, envir, enclos): could not find function \"tm_map\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "## Normalize tweets text\n",
    "tweet.corpus <- tm_map(tweet.corpus, content_transformer(removeNumbers))\n",
    "tweet.corpus <- tm_map(tweet.corpus, content_transformer(removePunctuation))\n",
    "tweet.corpus <- tm_map(tweet.corpus, content_transformer(stripWhitespace))\n",
    "tweet.corpus <- tm_map(tweet.corpus, content_transformer(tolower))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: tm\n",
      "Warning message in library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE, :\n",
      "\"there is no package called 'tm'\"Loading required package: slam\n",
      "Warning message in library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE, :\n",
      "\"there is no package called 'slam'\""
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in to.tdm(tweet.corpus): could not find function \"TermDocumentMatrix\"\n",
     "output_type": "error",
     "traceback": [
      "Error in to.tdm(tweet.corpus): could not find function \"TermDocumentMatrix\"\nTraceback:\n",
      "1. to.tdm(tweet.corpus)"
     ]
    }
   ],
   "source": [
    "## ----- Convert the corpus to a term document matrix\n",
    "to.tdm = function(corpus, sparse = 0.998){\n",
    "  require(tm)\n",
    "  ## Compute a term-document matrix and then \n",
    "  require(slam) # Sparse matrix package\n",
    "  tdm <- TermDocumentMatrix(corpus, control = list(stopwords = FALSE))\n",
    "  tdm <- removeSparseTerms(tdm, sparse)\n",
    "  tdm\n",
    "}\n",
    "tdm = to.tdm(tweet.corpus) # Create a term document matrix\n",
    "str(tdm) # Look at sparse tdm\n",
    "findFreqTerms(tdm, 2000) # Words that occur at least 2000 times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: slam\n",
      "Warning message in library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE, :\n",
      "\"there is no package called 'slam'\""
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in to.wf(tdm): could not find function \"row_sums\"\n",
     "output_type": "error",
     "traceback": [
      "Error in to.wf(tdm): could not find function \"row_sums\"\nTraceback:\n",
      "1. to.wf(tdm)"
     ]
    }
   ],
   "source": [
    "## Compute the word fequency from the tdm\n",
    "to.wf = function(tdm){\n",
    "  ## compute the word frequencies.\n",
    "  require(slam)\n",
    "  freq <- row_sums(tdm, na.rm = T)   \n",
    "  ## Sort the word frequency and build a dataframe\n",
    "  ## including the cumulative frequecy of the words.\n",
    "  freq <- sort(freq, decreasing = TRUE)\n",
    "  word.freq <- data.frame(word = factor(names(freq), levels = names(freq)), \n",
    "                          frequency = freq)\n",
    "  word.freq['Cumulative'] <- cumsum(word.freq['frequency'])/sum(word.freq$frequency)\n",
    "  word.freq\n",
    "}\n",
    "wf = to.wf(tdm)\n",
    "head(wf, n = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: ggplot2\n",
      "Warning message:\n",
      "\"package 'ggplot2' was built under R version 3.3.2\""
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in ggplot(wf[1:num, ], aes(word, frequency)): object 'wf' not found\n",
     "output_type": "error",
     "traceback": [
      "Error in ggplot(wf[1:num, ], aes(word, frequency)): object 'wf' not found\nTraceback:\n",
      "1. word.bar(wf)",
      "2. ggplot(wf[1:num, ], aes(word, frequency))   # at line 4-8 of file <text>"
     ]
    }
   ],
   "source": [
    "## Make a bar chart of the word frequency\n",
    "word.bar = function(wf, num = 50){\n",
    "  require(ggplot2)\n",
    "  ggplot(wf[1:num,], aes(word, frequency)) +\n",
    "    geom_bar(stat = 'identity') +\n",
    "    ggtitle('Frequency of common words') +\n",
    "    ylab('Frequency') +\n",
    "    theme(axis.text.x = element_text(angle = 90, hjust = 1))\n",
    "}\n",
    "word.bar(wf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in ggplot(wf[1:num, ], aes(word, Cumulative)): object 'wf' not found\n",
     "output_type": "error",
     "traceback": [
      "Error in ggplot(wf[1:num, ], aes(word, Cumulative)): object 'wf' not found\nTraceback:\n",
      "1. word.cdf(wf)",
      "2. ggplot(wf[1:num, ], aes(word, Cumulative))   # at line 4-8 of file <text>"
     ]
    }
   ],
   "source": [
    "## Make cumulative distribution plots of the most frequent words\n",
    "word.cdf = function(wf, num = 50){\n",
    "  require(ggplot2)\n",
    "  ggplot(wf[1:num,], aes(word, Cumulative)) +\n",
    "    geom_bar(stat = 'identity') +\n",
    "    ggtitle('Cumulative fraction of common words') +\n",
    "    ylab('Cumulative frequency') +\n",
    "    theme(axis.text.x = element_text(angle = 90, hjust = 1))\n",
    "}\n",
    "word.cdf(wf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'a'</li>\n",
       "\t<li>'about'</li>\n",
       "\t<li>'above'</li>\n",
       "\t<li>'actual'</li>\n",
       "\t<li>'after'</li>\n",
       "\t<li>'again'</li>\n",
       "\t<li>'against'</li>\n",
       "\t<li>'all'</li>\n",
       "\t<li>'alreadi'</li>\n",
       "\t<li>'also'</li>\n",
       "\t<li>'alway'</li>\n",
       "\t<li>'am'</li>\n",
       "\t<li>'amp'</li>\n",
       "\t<li>'an'</li>\n",
       "\t<li>'and'</li>\n",
       "\t<li>'ani'</li>\n",
       "\t<li>'anoth'</li>\n",
       "\t<li>'any'</li>\n",
       "\t<li>'anyth'</li>\n",
       "\t<li>'are'</li>\n",
       "\t<li>'aren\\'t'</li>\n",
       "\t<li>'around'</li>\n",
       "\t<li>'as'</li>\n",
       "\t<li>'at'</li>\n",
       "\t<li>'aww'</li>\n",
       "\t<li>'babi'</li>\n",
       "\t<li>'back'</li>\n",
       "\t<li>'be'</li>\n",
       "\t<li>'becaus'</li>\n",
       "\t<li>'because'</li>\n",
       "\t<li>'bed'</li>\n",
       "\t<li>'been'</li>\n",
       "\t<li>'befor'</li>\n",
       "\t<li>'before'</li>\n",
       "\t<li>'being'</li>\n",
       "\t<li>'below'</li>\n",
       "\t<li>'between'</li>\n",
       "\t<li>'birthday'</li>\n",
       "\t<li>'bit'</li>\n",
       "\t<li>'book'</li>\n",
       "\t<li>'both'</li>\n",
       "\t<li>'boy'</li>\n",
       "\t<li>'but'</li>\n",
       "\t<li>'by'</li>\n",
       "\t<li>'call'</li>\n",
       "\t<li>'can'</li>\n",
       "\t<li>'can\\'t'</li>\n",
       "\t<li>'cannot'</li>\n",
       "\t<li>'cant'</li>\n",
       "\t<li>'car'</li>\n",
       "\t<li>'check'</li>\n",
       "\t<li>'com'</li>\n",
       "\t<li>'come'</li>\n",
       "\t<li>'could'</li>\n",
       "\t<li>'couldn\\'t'</li>\n",
       "\t<li>'day'</li>\n",
       "\t<li>'did'</li>\n",
       "\t<li>'didn'</li>\n",
       "\t<li>'didn\\'t'</li>\n",
       "\t<li>'dinner'</li>\n",
       "\t<li>'do'</li>\n",
       "\t<li>'doe'</li>\n",
       "\t<li>'does'</li>\n",
       "\t<li>'doesn'</li>\n",
       "\t<li>'doesn\\'t'</li>\n",
       "\t<li>'doing'</li>\n",
       "\t<li>'don'</li>\n",
       "\t<li>'don\\'t'</li>\n",
       "\t<li>'done'</li>\n",
       "\t<li>'dont'</li>\n",
       "\t<li>'down'</li>\n",
       "\t<li>'during'</li>\n",
       "\t<li>'each'</li>\n",
       "\t<li>'eat'</li>\n",
       "\t<li>'end'</li>\n",
       "\t<li>'even'</li>\n",
       "\t<li>'ever'</li>\n",
       "\t<li>'everyon'</li>\n",
       "\t<li>'exam'</li>\n",
       "\t<li>'famili'</li>\n",
       "\t<li>'feel'</li>\n",
       "\t<li>'few'</li>\n",
       "\t<li>'final'</li>\n",
       "\t<li>'find'</li>\n",
       "\t<li>'first'</li>\n",
       "\t<li>'follow'</li>\n",
       "\t<li>'for'</li>\n",
       "\t<li>'for.'</li>\n",
       "\t<li>'found'</li>\n",
       "\t<li>'friday'</li>\n",
       "\t<li>'from'</li>\n",
       "\t<li>'further'</li>\n",
       "\t<li>'game'</li>\n",
       "\t<li>'get'</li>\n",
       "\t<li>'girl'</li>\n",
       "\t<li>'give'</li>\n",
       "\t<li>'gone'</li>\n",
       "\t<li>'gonna'</li>\n",
       "\t<li>'got'</li>\n",
       "\t<li>'gotta'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'a'\n",
       "\\item 'about'\n",
       "\\item 'above'\n",
       "\\item 'actual'\n",
       "\\item 'after'\n",
       "\\item 'again'\n",
       "\\item 'against'\n",
       "\\item 'all'\n",
       "\\item 'alreadi'\n",
       "\\item 'also'\n",
       "\\item 'alway'\n",
       "\\item 'am'\n",
       "\\item 'amp'\n",
       "\\item 'an'\n",
       "\\item 'and'\n",
       "\\item 'ani'\n",
       "\\item 'anoth'\n",
       "\\item 'any'\n",
       "\\item 'anyth'\n",
       "\\item 'are'\n",
       "\\item 'aren\\textbackslash{}'t'\n",
       "\\item 'around'\n",
       "\\item 'as'\n",
       "\\item 'at'\n",
       "\\item 'aww'\n",
       "\\item 'babi'\n",
       "\\item 'back'\n",
       "\\item 'be'\n",
       "\\item 'becaus'\n",
       "\\item 'because'\n",
       "\\item 'bed'\n",
       "\\item 'been'\n",
       "\\item 'befor'\n",
       "\\item 'before'\n",
       "\\item 'being'\n",
       "\\item 'below'\n",
       "\\item 'between'\n",
       "\\item 'birthday'\n",
       "\\item 'bit'\n",
       "\\item 'book'\n",
       "\\item 'both'\n",
       "\\item 'boy'\n",
       "\\item 'but'\n",
       "\\item 'by'\n",
       "\\item 'call'\n",
       "\\item 'can'\n",
       "\\item 'can\\textbackslash{}'t'\n",
       "\\item 'cannot'\n",
       "\\item 'cant'\n",
       "\\item 'car'\n",
       "\\item 'check'\n",
       "\\item 'com'\n",
       "\\item 'come'\n",
       "\\item 'could'\n",
       "\\item 'couldn\\textbackslash{}'t'\n",
       "\\item 'day'\n",
       "\\item 'did'\n",
       "\\item 'didn'\n",
       "\\item 'didn\\textbackslash{}'t'\n",
       "\\item 'dinner'\n",
       "\\item 'do'\n",
       "\\item 'doe'\n",
       "\\item 'does'\n",
       "\\item 'doesn'\n",
       "\\item 'doesn\\textbackslash{}'t'\n",
       "\\item 'doing'\n",
       "\\item 'don'\n",
       "\\item 'don\\textbackslash{}'t'\n",
       "\\item 'done'\n",
       "\\item 'dont'\n",
       "\\item 'down'\n",
       "\\item 'during'\n",
       "\\item 'each'\n",
       "\\item 'eat'\n",
       "\\item 'end'\n",
       "\\item 'even'\n",
       "\\item 'ever'\n",
       "\\item 'everyon'\n",
       "\\item 'exam'\n",
       "\\item 'famili'\n",
       "\\item 'feel'\n",
       "\\item 'few'\n",
       "\\item 'final'\n",
       "\\item 'find'\n",
       "\\item 'first'\n",
       "\\item 'follow'\n",
       "\\item 'for'\n",
       "\\item 'for.'\n",
       "\\item 'found'\n",
       "\\item 'friday'\n",
       "\\item 'from'\n",
       "\\item 'further'\n",
       "\\item 'game'\n",
       "\\item 'get'\n",
       "\\item 'girl'\n",
       "\\item 'give'\n",
       "\\item 'gone'\n",
       "\\item 'gonna'\n",
       "\\item 'got'\n",
       "\\item 'gotta'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'a'\n",
       "2. 'about'\n",
       "3. 'above'\n",
       "4. 'actual'\n",
       "5. 'after'\n",
       "6. 'again'\n",
       "7. 'against'\n",
       "8. 'all'\n",
       "9. 'alreadi'\n",
       "10. 'also'\n",
       "11. 'alway'\n",
       "12. 'am'\n",
       "13. 'amp'\n",
       "14. 'an'\n",
       "15. 'and'\n",
       "16. 'ani'\n",
       "17. 'anoth'\n",
       "18. 'any'\n",
       "19. 'anyth'\n",
       "20. 'are'\n",
       "21. 'aren\\'t'\n",
       "22. 'around'\n",
       "23. 'as'\n",
       "24. 'at'\n",
       "25. 'aww'\n",
       "26. 'babi'\n",
       "27. 'back'\n",
       "28. 'be'\n",
       "29. 'becaus'\n",
       "30. 'because'\n",
       "31. 'bed'\n",
       "32. 'been'\n",
       "33. 'befor'\n",
       "34. 'before'\n",
       "35. 'being'\n",
       "36. 'below'\n",
       "37. 'between'\n",
       "38. 'birthday'\n",
       "39. 'bit'\n",
       "40. 'book'\n",
       "41. 'both'\n",
       "42. 'boy'\n",
       "43. 'but'\n",
       "44. 'by'\n",
       "45. 'call'\n",
       "46. 'can'\n",
       "47. 'can\\'t'\n",
       "48. 'cannot'\n",
       "49. 'cant'\n",
       "50. 'car'\n",
       "51. 'check'\n",
       "52. 'com'\n",
       "53. 'come'\n",
       "54. 'could'\n",
       "55. 'couldn\\'t'\n",
       "56. 'day'\n",
       "57. 'did'\n",
       "58. 'didn'\n",
       "59. 'didn\\'t'\n",
       "60. 'dinner'\n",
       "61. 'do'\n",
       "62. 'doe'\n",
       "63. 'does'\n",
       "64. 'doesn'\n",
       "65. 'doesn\\'t'\n",
       "66. 'doing'\n",
       "67. 'don'\n",
       "68. 'don\\'t'\n",
       "69. 'done'\n",
       "70. 'dont'\n",
       "71. 'down'\n",
       "72. 'during'\n",
       "73. 'each'\n",
       "74. 'eat'\n",
       "75. 'end'\n",
       "76. 'even'\n",
       "77. 'ever'\n",
       "78. 'everyon'\n",
       "79. 'exam'\n",
       "80. 'famili'\n",
       "81. 'feel'\n",
       "82. 'few'\n",
       "83. 'final'\n",
       "84. 'find'\n",
       "85. 'first'\n",
       "86. 'follow'\n",
       "87. 'for'\n",
       "88. 'for.'\n",
       "89. 'found'\n",
       "90. 'friday'\n",
       "91. 'from'\n",
       "92. 'further'\n",
       "93. 'game'\n",
       "94. 'get'\n",
       "95. 'girl'\n",
       "96. 'give'\n",
       "97. 'gone'\n",
       "98. 'gonna'\n",
       "99. 'got'\n",
       "100. 'gotta'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  [1] \"a\"        \"about\"    \"above\"    \"actual\"   \"after\"    \"again\"   \n",
       "  [7] \"against\"  \"all\"      \"alreadi\"  \"also\"     \"alway\"    \"am\"      \n",
       " [13] \"amp\"      \"an\"       \"and\"      \"ani\"      \"anoth\"    \"any\"     \n",
       " [19] \"anyth\"    \"are\"      \"aren't\"   \"around\"   \"as\"       \"at\"      \n",
       " [25] \"aww\"      \"babi\"     \"back\"     \"be\"       \"becaus\"   \"because\" \n",
       " [31] \"bed\"      \"been\"     \"befor\"    \"before\"   \"being\"    \"below\"   \n",
       " [37] \"between\"  \"birthday\" \"bit\"      \"book\"     \"both\"     \"boy\"     \n",
       " [43] \"but\"      \"by\"       \"call\"     \"can\"      \"can't\"    \"cannot\"  \n",
       " [49] \"cant\"     \"car\"      \"check\"    \"com\"      \"come\"     \"could\"   \n",
       " [55] \"couldn't\" \"day\"      \"did\"      \"didn\"     \"didn't\"   \"dinner\"  \n",
       " [61] \"do\"       \"doe\"      \"does\"     \"doesn\"    \"doesn't\"  \"doing\"   \n",
       " [67] \"don\"      \"don't\"    \"done\"     \"dont\"     \"down\"     \"during\"  \n",
       " [73] \"each\"     \"eat\"      \"end\"      \"even\"     \"ever\"     \"everyon\" \n",
       " [79] \"exam\"     \"famili\"   \"feel\"     \"few\"      \"final\"    \"find\"    \n",
       " [85] \"first\"    \"follow\"   \"for\"      \"for.\"     \"found\"    \"friday\"  \n",
       " [91] \"from\"     \"further\"  \"game\"     \"get\"      \"girl\"     \"give\"    \n",
       " [97] \"gone\"     \"gonna\"    \"got\"      \"gotta\"   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## ----------------------------------------------------------\n",
    "## -----------Stop words -------------------------------------\n",
    "##\n",
    "## Load stop words from a file and ensure they are \n",
    "stopWords = read.csv('stopwords.csv', header = TRUE, stringsAsFactors = FALSE)\n",
    "stopWords = unique(stopWords) # Ensure the list is unique\n",
    "stopWords[1:100,] # Look at the first 100 stop words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in eval(expr, envir, enclos): could not find function \"tm_map\"\n",
     "output_type": "error",
     "traceback": [
      "Error in eval(expr, envir, enclos): could not find function \"tm_map\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "## Remove the stop words from the corpus\n",
    "tweet.corpus <- tm_map(tweet.corpus, removeWords, stopWords[, 'words'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in eval(expr, envir, enclos): could not find function \"tm_map\"\n",
     "output_type": "error",
     "traceback": [
      "Error in eval(expr, envir, enclos): could not find function \"tm_map\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "## Remove the stop words from the corpus\n",
    "tweet.corpus <- tm_map(tweet.corpus, removeWords, stopWords[, 'words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: tm\n",
      "Warning message in library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE, :\n",
      "\"there is no package called 'tm'\"Loading required package: slam\n",
      "Warning message in library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE, :\n",
      "\"there is no package called 'slam'\""
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in to.tdm(tweet.corpus): could not find function \"TermDocumentMatrix\"\n",
     "output_type": "error",
     "traceback": [
      "Error in to.tdm(tweet.corpus): could not find function \"TermDocumentMatrix\"\nTraceback:\n",
      "1. to.tdm(tweet.corpus)"
     ]
    }
   ],
   "source": [
    "## View the results\n",
    "tdm = to.tdm(tweet.corpus) # Create a term document matrix\n",
    "findFreqTerms(tdm, 2000) # Words that occur at least 2000 times\n",
    "wf = to.wf(tdm)  # Compute word fequency\n",
    "head(wf, n = 10)  # Look at the most common words\n",
    "word.bar(wf) # Plot word frequency\n",
    "word.cdf(wf) # Plot cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: SnowballC\n",
      "Warning message in library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE, :\n",
      "\"there is no package called 'SnowballC'\""
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in eval(expr, envir, enclos): could not find function \"tm_map\"\n",
     "output_type": "error",
     "traceback": [
      "Error in eval(expr, envir, enclos): could not find function \"tm_map\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "## --------------------------------------------------\n",
    "## ------------ Stem the words ----------------------\n",
    "##\n",
    "## Use the porter stemmer in Snowball package\n",
    "##\n",
    "require(SnowballC) ## For Porter stemming words\n",
    "tweet.corpus <- tm_map(tweet.corpus, stemDocument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: tm\n",
      "Warning message in library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE, :\n",
      "\"there is no package called 'tm'\"Loading required package: slam\n",
      "Warning message in library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE, :\n",
      "\"there is no package called 'slam'\""
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in to.tdm(tweet.corpus, sparse = 0.99): could not find function \"TermDocumentMatrix\"\n",
     "output_type": "error",
     "traceback": [
      "Error in to.tdm(tweet.corpus, sparse = 0.99): could not find function \"TermDocumentMatrix\"\nTraceback:\n",
      "1. to.tdm(tweet.corpus, sparse = 0.99)"
     ]
    }
   ],
   "source": [
    "## View the results\n",
    "tdm = to.tdm(tweet.corpus, sparse = 0.99) # Create a term document matrix\n",
    "findFreqTerms(tdm, 2000) # Words that occur at least 2000 times\n",
    "wf = to.wf(tdm)  # Compute word fequency\n",
    "head(wf, n = 10)  # Look at the most common words\n",
    "word.bar(wf) # Plot word frequency\n",
    "word.cdf(wf) # Plot cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: tm\n",
      "Warning message in library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE, :\n",
      "\"there is no package called 'tm'\"Loading required package: slam\n",
      "Warning message in library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE, :\n",
      "\"there is no package called 'slam'\""
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in to.tdm(tweet.corpus, sparse = 0.99): could not find function \"TermDocumentMatrix\"\n",
     "output_type": "error",
     "traceback": [
      "Error in to.tdm(tweet.corpus, sparse = 0.99): could not find function \"TermDocumentMatrix\"\nTraceback:\n",
      "1. to.tdm(tweet.corpus, sparse = 0.99)"
     ]
    }
   ],
   "source": [
    "## View the results\n",
    "tdm = to.tdm(tweet.corpus, sparse = 0.99) # Create a term document matrix\n",
    "findFreqTerms(tdm, 2000) # Words that occur at least 2000 times\n",
    "wf = to.wf(tdm)  # Compute word fequency\n",
    "head(wf, n = 10)  # Look at the most common words\n",
    "word.bar(wf) # Plot word frequency\n",
    "word.cdf(wf) # Plot cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in eval(expr, envir, enclos): could not find function \"create_container\"\n",
     "output_type": "error",
     "traceback": [
      "Error in eval(expr, envir, enclos): could not find function \"create_container\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "## Create the a container for the tdm and label\n",
    "tweet.cont = create_container(tdm.tools,tweets$sentiment, trainSize = 1:120000, virgin=TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in eval(expr, envir, enclos): could not find function \"train_model\"\n",
     "output_type": "error",
     "traceback": [
      "Error in eval(expr, envir, enclos): could not find function \"train_model\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "## Compute a logistic regresson model for sentiment classification\n",
    "tweet.glmnet <- train_model(tweet.cont, \"GLMNET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in eval(expr, envir, enclos): could not find function \"classify_model\"\n",
     "output_type": "error",
     "traceback": [
      "Error in eval(expr, envir, enclos): could not find function \"classify_model\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "## Test classification\n",
    "tweet.class = classify_model(tweet.cont, tweet.glmnet)\n",
    "tweet.metrics = create_analytics(tweet.cont, tweet.class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in eval(expr, envir, enclos): object 'tweet.metrics' not found\n",
     "output_type": "error",
     "traceback": [
      "Error in eval(expr, envir, enclos): object 'tweet.metrics' not found\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "## Examine some raw metrics\n",
    "tweet.metrics@label_summary\n",
    "cbind(head(tweet.metrics.TfIdf@document_summary, n = 10), head(tweets$sentiment, n = 10))\n",
    "cbind(head(tweet.metrics@document_summary, n = 10), head(tweets$sentiment, n = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in eval(expr, envir, enclos): could not find function \"create_precisionRecallSummary\"\n",
     "output_type": "error",
     "traceback": [
      "Error in eval(expr, envir, enclos): could not find function \"create_precisionRecallSummary\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "create_precisionRecallSummary(tweet.cont, tweet.class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in eval(expr, envir, enclos): could not find function \"create_matrix\"\n",
     "output_type": "error",
     "traceback": [
      "Error in eval(expr, envir, enclos): could not find function \"create_matrix\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------\n",
    "## Compute TFIdf weighted tdm\n",
    "## Compute a tdm\n",
    "tdm.tools2 = create_matrix(tweet.frame$tweets, language = \"english\", removeNumbers = FALSE,\n",
    "                          stemWords = FALSE, removeSparseTerms = .998, \n",
    "                          removeStopwords = FALSE, stripWhitespace = FALSE,\n",
    "                          toLower = FALSE, weighting = tm::weightTfIdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in eval(expr, envir, enclos): could not find function \"create_container\"\n",
     "output_type": "error",
     "traceback": [
      "Error in eval(expr, envir, enclos): could not find function \"create_container\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "## Create the a container for the TfIdf weighted tdm and label\n",
    "tweet.cont = create_container(tdm.tools2,tweets$sentiment, trainSize = 1:120000, virgin=TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in eval(expr, envir, enclos): could not find function \"train_model\"\n",
     "output_type": "error",
     "traceback": [
      "Error in eval(expr, envir, enclos): could not find function \"train_model\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "## Compute a logistic regresson model for sentiment classification\n",
    "tweet.glmnet.TfIdf <- train_model(tweet.cont,\"GLMNET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in eval(expr, envir, enclos): could not find function \"classify_model\"\n",
     "output_type": "error",
     "traceback": [
      "Error in eval(expr, envir, enclos): could not find function \"classify_model\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "## Test classification\n",
    "tweet.class.TfIdf = classify_model(tweet.cont, tweet.glmnet.TfIdf)\n",
    "tweet.metrics.TfIdf = create_analytics(tweet.cont, tweet.class.TfIdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in eval(expr, envir, enclos): object 'tweet.metrics.TfIdf' not found\n",
     "output_type": "error",
     "traceback": [
      "Error in eval(expr, envir, enclos): object 'tweet.metrics.TfIdf' not found\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "## Examine some raw metrics\n",
    "tweet.metrics.TfIdf@label_summary\n",
    "results = head(tweet.metrics.TfIdf@document_summary, n = 20)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in eval(expr, envir, enclos): could not find function \"create_precisionRecallSummary\"\n",
     "output_type": "error",
     "traceback": [
      "Error in eval(expr, envir, enclos): could not find function \"create_precisionRecallSummary\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "## Look at the confusion matrix and compare to the unweighte tdf model\n",
    "create_precisionRecallSummary(tweet.cont, tweet.class.TfIdf)\n",
    "create_precisionRecallSummary(tweet.cont, tweet.class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in library(tm): there is no package called 'tm'\n",
     "output_type": "error",
     "traceback": [
      "Error in library(tm): there is no package called 'tm'\nTraceback:\n",
      "1. library(tm)",
      "2. stop(txt, domain = NA)"
     ]
    }
   ],
   "source": [
    "## ---------------------------------------------------\n",
    "## ---------- Exploring Term Document Matrix\n",
    "##\n",
    "## Load the data set as a vector corpus of 20 documents\n",
    "library(tm)\n",
    "data(crude)\n",
    "writeLines(as.character(crude[[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in eval(expr, envir, enclos): could not find function \"TermDocumentMatrix\"\n",
     "output_type": "error",
     "traceback": [
      "Error in eval(expr, envir, enclos): could not find function \"TermDocumentMatrix\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "## Compute the term document matrix\n",
    "crude.tdm = TermDocumentMatrix(crude, control = list(removePunctuation = TRUE,\n",
    "                                                     tolower = TRUE,\n",
    "                                                     removePunctuation = TRUE,\n",
    "                                                     removeNumbers = TRUE,\n",
    "                                                     stopwords = TRUE,\n",
    "                                                     stemming = TRUE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in eval(expr, envir, enclos): could not find function \"inspect\"\n",
     "output_type": "error",
     "traceback": [
      "Error in eval(expr, envir, enclos): could not find function \"inspect\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "## Have a look at the tdm \n",
    "inspect(crude.tdm[202:205, 1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in eval(expr, envir, enclos): could not find function \"findFreqTerms\"\n",
     "output_type": "error",
     "traceback": [
      "Error in eval(expr, envir, enclos): could not find function \"findFreqTerms\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "## Which terms occur 10 times or more?\n",
    "crudeTDMHighFreq <- findFreqTerms(crude.tdm, 10, Inf)\n",
    "crudeTDMHighFreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in eval(expr, envir, enclos): could not find function \"inspect\"\n",
     "output_type": "error",
     "traceback": [
      "Error in eval(expr, envir, enclos): could not find function \"inspect\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "# Do these terms show up in the first 5 documents?\n",
    "inspect(crude.tdm[crudeTDMHighFreq, 1:5]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in library(topicmodels): there is no package called 'topicmodels'\n",
     "output_type": "error",
     "traceback": [
      "Error in library(topicmodels): there is no package called 'topicmodels'\nTraceback:\n",
      "1. library(topicmodels)",
      "2. stop(txt, domain = NA)"
     ]
    }
   ],
   "source": [
    "## -------------------------------------------------------\n",
    "## Apply a topic model to the news articles\\\n",
    "##\n",
    "##load topic models library\n",
    "library(topicmodels)\n",
    "\n",
    "#Set parameters for Gibbs sampling\n",
    "burnin <- 4000\n",
    "iter <- 2000\n",
    "thin <- 500\n",
    "seed <-list(2003,5,63,100001,765)\n",
    "nstart <- 5\n",
    "best <- TRUE\n",
    "\n",
    "#Number of topics\n",
    "k <- 5\n",
    "\n",
    "## Compute the LDA model\n",
    "crude.dtm = DocumentTermMatrix(crude, control = list(removePunctuation = TRUE,\n",
    "                                                     stopwords = TRUE))\n",
    "crude.dtm  ## Check the drm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in eval(expr, envir, enclos): could not find function \"LDA\"\n",
     "output_type": "error",
     "traceback": [
      "Error in eval(expr, envir, enclos): could not find function \"LDA\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "ldaOut = LDA(crude.dtm, k, method= \"Gibbs\", control = list(nstart = nstart, seed = seed, best = best, burnin = burnin, iter = iter, thin=thin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in as.matrix(topics(ldaOut)): could not find function \"topics\"\n",
     "output_type": "error",
     "traceback": [
      "Error in as.matrix(topics(ldaOut)): could not find function \"topics\"\nTraceback:\n",
      "1. as.matrix(topics(ldaOut))"
     ]
    }
   ],
   "source": [
    "## Examine the topics\n",
    "ldaOut.topics <- as.matrix(topics(ldaOut))\n",
    "head(ldaOut.topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in terms(ldaOut, 6): object 'ldaOut' not found\n",
     "output_type": "error",
     "traceback": [
      "Error in terms(ldaOut, 6): object 'ldaOut' not found\nTraceback:\n",
      "1. as.matrix(terms(ldaOut, 6))",
      "2. terms(ldaOut, 6)"
     ]
    }
   ],
   "source": [
    "## And the terms\n",
    "ldaOut.terms <- as.matrix(terms(ldaOut,6))\n",
    "head(ldaOut.terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in as.data.frame(ldaOut@gamma): object 'ldaOut' not found\n",
     "output_type": "error",
     "traceback": [
      "Error in as.data.frame(ldaOut@gamma): object 'ldaOut' not found\nTraceback:\n",
      "1. as.data.frame(ldaOut@gamma)"
     ]
    }
   ],
   "source": [
    "#probabilities associated with each topic assignment\n",
    "topicProbabilities <- as.data.frame(ldaOut@gamma)\n",
    "head(topicProbabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in nrow(crude.dtm): object 'crude.dtm' not found\n",
     "output_type": "error",
     "traceback": [
      "Error in nrow(crude.dtm): object 'crude.dtm' not found\nTraceback:\n",
      "1. lapply(1:nrow(crude.dtm), function(x) sort(topicProbabilities[x, \n .     ])[k]/sort(topicProbabilities[x, ])[k - 1])",
      "2. nrow(crude.dtm)"
     ]
    }
   ],
   "source": [
    "#Find relative importance of top 2 topics\n",
    "topic1ToTopic2 <- lapply(1:nrow(crude.dtm),function(x)\n",
    "  sort(topicProbabilities[x,])[k]/sort(topicProbabilities[x,])[k-1])\n",
    "unlist(topic1ToTopic2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in nrow(crude.dtm): object 'crude.dtm' not found\n",
     "output_type": "error",
     "traceback": [
      "Error in nrow(crude.dtm): object 'crude.dtm' not found\nTraceback:\n",
      "1. lapply(1:nrow(crude.dtm), function(x) sort(topicProbabilities[x, \n .     ])[k - 1]/sort(topicProbabilities[x, ])[k - 2])",
      "2. nrow(crude.dtm)"
     ]
    }
   ],
   "source": [
    "#Find relative importance of second and third most important topics\n",
    "topic2ToTopic3 <- lapply(1:nrow(crude.dtm),function(x)\n",
    "  sort(topicProbabilities[x,])[k-1]/sort(topicProbabilities[x,])[k-2])\n",
    "topic2ToTopic3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
